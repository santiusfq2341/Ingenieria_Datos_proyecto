{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f8d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset generado: C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\retenciones\\retenciones_aug2025_4.csv\n",
      "           id_cliente No_Secuencial_Comprobante_Retencion Fecha_Emision_Comprobante_Retencion Tipo_Rendimiento_Financiero  Base_Imponible_Renta  Monto_Retencion_Renta\n",
      "50196HCOMP06059232914                              662500                           27/8/2025                        323A               1305.92                  26.12\n",
      "88835YCOMP93025476357                              662501                           16/8/2025                        324B                  4.57                   0.05\n",
      "12234UCOMP35907796557                              662502                           26/8/2025                        324B                 38.52                   0.39\n",
      "32530DCOMP83391832431                              662503                           30/8/2025                        323A                271.92                   5.44\n",
      "49613LCOMP88423407256                              662504                            4/8/2025                        324B                  4.48                   0.04\n",
      "00287KCOMP42090304639                              662505                           27/8/2025                        323A                 76.64                   1.53\n",
      "91056LCOMP45045808051                              662506                           20/8/2025                        323A                  3.60                   0.07\n",
      "97173YCOMP87768677875                              662507                           29/8/2025                        323A                  2.61                   0.05\n",
      "86646ACOMP00566090561                              662508                            2/8/2025                        323A               9553.53                 191.07\n",
      "20517ACOMP61884111757                              662509                           19/8/2025                        324B                 71.98                   0.72\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from datetime import date, timedelta\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------ Parámetros ------------------\n",
    "N = 1500\n",
    "archivo = \"_4\"                       # filas a generar en esta corrida\n",
    "OUTPUT_PATH = fr\"C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\retenciones\\retenciones_aug2025{archivo}.csv\"\n",
    "STATE_PATH = fr\"C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Arch_Control\\retenciones_state.json\"  # guarda el último secuencial usado\n",
    "START_SECUENCIAL = 650000      # valor inicial si no existe estado previo\n",
    "ERROR_PROBABILITY = 0.15       # prob. de aplicar tasa incorrecta (0.5%, 3%, 5%)\n",
    "SEED = 42                      # fija aleatoriedad reproducible; cámbialo o quítalo\n",
    "# ------------------------------------------------\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def load_last_secuencial(state_path: str, fallback: int) -> int:\n",
    "    \"\"\"\n",
    "    Lee el último secuencial usado desde un archivo JSON.\n",
    "    Si no existe, retorna fallback - 1 para que el primero sea 'fallback'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(state_path):\n",
    "        return fallback - 1\n",
    "    try:\n",
    "        with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return int(data.get(\"last_no_secuencial\", fallback - 1))\n",
    "    except Exception:\n",
    "        # Si el archivo está corrupto, reinicia\n",
    "        return fallback - 1\n",
    "\n",
    "def save_last_secuencial(state_path: str, last_value: int) -> None:\n",
    "    \"\"\"Guarda el último secuencial usado en JSON.\"\"\"\n",
    "    with open(state_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"last_no_secuencial\": last_value}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def next_secuenciales(n: int, state_path: str, fallback_start: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retorna n secuenciales *nuevos* de 6 dígitos, consecutivos,\n",
    "    continuando desde el último valor persistido.\n",
    "    \"\"\"\n",
    "    last = load_last_secuencial(state_path, fallback_start)\n",
    "    seqs = [f\"{v:06d}\" for v in range(last + 1, last + 1 + n)]\n",
    "    save_last_secuencial(state_path, last + n)\n",
    "    return seqs\n",
    "\n",
    "def random_id_cliente() -> str:\n",
    "    \"\"\"\n",
    "    Genera un ID alfanumérico no repetido (en la corrida actual),\n",
    "    independiente del secuencial, con patrón:\n",
    "      5 dígitos + 1 letra mayúscula + \"COMP\" + 11 dígitos\n",
    "    Ej: 45838ACOMP00064131600\n",
    "    \"\"\"\n",
    "    digits5 = f\"{random.randint(0, 99999):05d}\"\n",
    "    letter = random.choice(string.ascii_uppercase)\n",
    "    tail11 = f\"{random.randint(0, 99999999999):011d}\"\n",
    "    return f\"{digits5}{letter}COMP{tail11}\"\n",
    "\n",
    "def random_august_2025_date() -> str:\n",
    "    \"\"\"Fecha aleatoria entre 1/8/2025 y 31/8/2025, formato D/M/YYYY (sin ceros a la izquierda).\"\"\"\n",
    "    start = date(2025, 8, 1)\n",
    "    d = start + timedelta(days=random.randint(0, 30))\n",
    "    return f\"{d.day}/{d.month}/{d.year}\"\n",
    "\n",
    "def random_base_imponible() -> Decimal:\n",
    "    \"\"\"\n",
    "    Valor con hasta 2 decimales donde (entera + decimales) tenga entre 3 y 6 dígitos.\n",
    "    Estrategia:\n",
    "      - dígitos enteros = 1..4 (con 2 decimales => total 3..6)\n",
    "      - 2 decimales\n",
    "    \"\"\"\n",
    "    int_digits = random.choice([1, 2, 3, 4])\n",
    "    if int_digits == 1:\n",
    "        int_part = random.randint(1, 9)\n",
    "    elif int_digits == 2:\n",
    "        int_part = random.randint(10, 99)\n",
    "    elif int_digits == 3:\n",
    "        int_part = random.randint(100, 999)\n",
    "    else:\n",
    "        int_part = random.randint(1000, 9999)\n",
    "\n",
    "    dec_part = random.randint(0, 99)\n",
    "    value = Decimal(int_part) + Decimal(dec_part) / Decimal(100)\n",
    "    return value.quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP)\n",
    "\n",
    "def monto_retencion(base: Decimal, tipo: str, error_probability: float) -> Decimal:\n",
    "    \"\"\"\n",
    "    Regla correcta:\n",
    "      - 323A -> 2%\n",
    "      - 324B -> 1%\n",
    "    Con probabilidad 'error_probability', usamos una tasa incorrecta\n",
    "    (0.5%, 3% o 5%) para introducir errores en el dataset.\n",
    "    Devuelve el monto (2 decimales).\n",
    "    \"\"\"\n",
    "    expected_rate = Decimal(\"0.02\") if tipo == \"323A\" else Decimal(\"0.01\")\n",
    "    wrong_rates = [Decimal(\"0.005\"), Decimal(\"0.03\"), Decimal(\"0.05\")]\n",
    "\n",
    "    if random.random() < error_probability:\n",
    "        rate = random.choice(wrong_rates)\n",
    "    else:\n",
    "        rate = expected_rate\n",
    "\n",
    "    return (base * rate).quantize(Decimal(\"0.01\"), rounding=ROUND_HALF_UP)\n",
    "\n",
    "def main():\n",
    "    # 1) Secuenciales que continúan desde la última corrida\n",
    "    secuenciales = next_secuenciales(N, STATE_PATH, START_SECUENCIAL)\n",
    "\n",
    "    # 2) id_cliente aleatorios y únicos (en esta corrida)\n",
    "    id_clientes = set()\n",
    "    while len(id_clientes) < N:\n",
    "        id_clientes.add(random_id_cliente())\n",
    "    id_clientes = list(id_clientes)\n",
    "\n",
    "    # 3) Resto de campos y armado del DataFrame\n",
    "    tipos = [\"323A\", \"324B\"]\n",
    "    rows = []\n",
    "    for i in range(N):\n",
    "        tipo = random.choice(tipos)\n",
    "        base = random_base_imponible()\n",
    "        monto = monto_retencion(base, tipo, ERROR_PROBABILITY)\n",
    "        rows.append({\n",
    "            \"id_cliente\": id_clientes[i],\n",
    "            \"No_Secuencial_Comprobante_Retencion\": secuenciales[i],\n",
    "            \"Fecha_Emision_Comprobante_Retencion\": random_august_2025_date(),\n",
    "            \"Tipo_Rendimiento_Financiero\": tipo,\n",
    "            \"Base_Imponible_Renta\": float(base),\n",
    "            \"Monto_Retencion_Renta\": float(monto),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Orden garantizado de las 6 columnas pedidas\n",
    "    df = df[\n",
    "        [\n",
    "            \"id_cliente\",\n",
    "            \"No_Secuencial_Comprobante_Retencion\",\n",
    "            \"Fecha_Emision_Comprobante_Retencion\",\n",
    "            \"Tipo_Rendimiento_Financiero\",\n",
    "            \"Base_Imponible_Renta\",\n",
    "            \"Monto_Retencion_Renta\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Dataset generado: {OUTPUT_PATH}\")\n",
    "    print(df.head(10).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ff1cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo generado: C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\retenciones\\retenciones_aug2025_4.csv\n",
      "           id_cliente  cod_tipo_id  cod_provincia\n",
      "50196HCOMP06059232914           14             16\n",
      "88835YCOMP93025476357           13             12\n",
      "12234UCOMP35907796557           14             10\n",
      "32530DCOMP83391832431           13             13\n",
      "49613LCOMP88423407256           13             21\n",
      "00287KCOMP42090304639           12              5\n",
      "91056LCOMP45045808051           14             19\n",
      "97173YCOMP87768677875           13             17\n",
      "86646ACOMP00566090561           12             15\n",
      "20517ACOMP61884111757           12              1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ruta del archivo original generado previamente\n",
    "INPUT_PATH = OUTPUT_PATH\n",
    "OUTPUT_PATH_2 = fr\"C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\info_clientes\\id_clientes_codigos{archivo}.csv\"\n",
    "\n",
    "def main():\n",
    "    # Verificar que exista el archivo base\n",
    "    if not os.path.exists(INPUT_PATH):\n",
    "        raise FileNotFoundError(f\"No se encontró {INPUT_PATH}, genera primero el dataset base.\")\n",
    "\n",
    "    # Leer archivo original\n",
    "    df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "    # Extraer columna id_cliente\n",
    "    result = pd.DataFrame({\"id_cliente\": df[\"id_cliente\"]})\n",
    "\n",
    "    # Añadir columna cod_tipo_id con valores aleatorios {12, 13, 14}\n",
    "    result[\"cod_tipo_id\"] = np.random.choice([12, 13, 14], size=len(result))\n",
    "\n",
    "    # Lista válida de provincias: 1..25 y 30 (excluye 26–29)\n",
    "    valid_provincias = list(range(1, 26)) + [30]\n",
    "    result[\"cod_provincia\"] = np.random.choice(valid_provincias, size=len(result))\n",
    "\n",
    "    # Guardar resultado en nuevo CSV\n",
    "    result.to_csv(OUTPUT_PATH_2, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅ Archivo generado: {OUTPUT_PATH}\")\n",
    "    print(result.head(10).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cef8a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] retenciones_aug2025_1.csv -> 5000 filas insertadas (duplicados ignorados)\n",
      "[OK] retenciones_aug2025_2.csv -> 7000 filas insertadas (duplicados ignorados)\n",
      "[OK] retenciones_aug2025_3.csv -> 500 filas insertadas (duplicados ignorados)\n",
      "[OK] retenciones_aug2025_4.csv -> 1500 filas insertadas (duplicados ignorados)\n",
      "\n",
      "Resumen: Archivos procesados=4, Filas insertadas=14000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import mysql.connector as mc\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== CONFIG ======\n",
    "CSV_DIR = r\"C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\retenciones\"\n",
    "BATCH_SIZE = 2000  # inserta en lotes para mejorar rendimiento\n",
    "# ====================\n",
    "\n",
    "# === 2. Conexión a MySQL ===\n",
    "conn = mc.connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    password=\"miclave\",\n",
    "    database=\"retenciones_mensuales\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Sentencia SOLO INSERTAR (ignora duplicados)\n",
    "SQL_INSERT = \"\"\"\n",
    "INSERT IGNORE INTO retenciones (\n",
    "  id_cliente,\n",
    "  no_secuencial_retencion,\n",
    "  fecha_emision,\n",
    "  cod_retencion,\n",
    "  base_imponible,\n",
    "  monto_retencion\n",
    ") VALUES (%s,%s,%s,%s,%s,%s)\n",
    "\"\"\"\n",
    "\n",
    "def parse_fecha(fecha_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Convierte fechas tipo D/M/YYYY o DD/MM/YYYY a 'YYYY-MM-DD'.\n",
    "    Si ya viene en formato ISO, lo respeta.\n",
    "    \"\"\"\n",
    "    if pd.isna(fecha_str):\n",
    "        return None\n",
    "    s = str(fecha_str).strip()\n",
    "\n",
    "    # Ya en formato ISO YYYY-MM-DD\n",
    "    try:\n",
    "        if len(s) == 10 and s[4] == \"-\" and s[7] == \"-\":\n",
    "            datetime.strptime(s, \"%Y-%m-%d\")\n",
    "            return s\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Intento manual: D/M/YYYY\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "        if len(parts) == 3:\n",
    "            d, m, y = parts\n",
    "            d = int(d)\n",
    "            m = int(m)\n",
    "            y = int(y)\n",
    "            return f\"{y:04d}-{m:02d}-{d:02d}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: pandas\n",
    "    try:\n",
    "        dt = pd.to_datetime(s, dayfirst=True, errors=\"raise\")\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def cargar_csv_a_mysql(ruta_csv: str) -> int:\n",
    "    \"\"\"\n",
    "    Lee un CSV y lo inserta en MySQL.\n",
    "    Devuelve el número de filas insertadas (los duplicados se ignoran).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "\n",
    "    # Mapeo de columnas CSV -> DB\n",
    "    col_map = {\n",
    "        \"id_cliente\": \"id_cliente\",\n",
    "        \"No_Secuencial_Comprobante_Retencion\": \"no_secuencial_retencion\",\n",
    "        \"Fecha_Emision_Comprobante_Retencion\": \"fecha_emision\",\n",
    "        \"Tipo_Rendimiento_Financiero\": \"cod_retencion\",\n",
    "        \"Base_Imponible_Renta\": \"base_imponible\",\n",
    "        \"Monto_Retencion_Renta\": \"monto_retencion\",\n",
    "    }\n",
    "\n",
    "    # Verifica columnas necesarias\n",
    "    faltantes = [c for c in col_map.keys() if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"Columnas faltantes en {os.path.basename(ruta_csv)}: {faltantes}\")\n",
    "\n",
    "    # Construir DataFrame con nombres destino\n",
    "    d = pd.DataFrame()\n",
    "    for src, dst in col_map.items():\n",
    "        d[dst] = df[src]\n",
    "\n",
    "    # Transformaciones\n",
    "    d[\"fecha_emision\"] = d[\"fecha_emision\"].apply(parse_fecha)\n",
    "    d[\"no_secuencial_retencion\"] = d[\"no_secuencial_retencion\"].astype(str).str.strip().str.zfill(6)\n",
    "    d[\"cod_retencion\"] = d[\"cod_retencion\"].astype(str).str.strip().str[:5]\n",
    "\n",
    "    def to_float(x):\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        s = str(x).strip().replace(\",\", \"\")\n",
    "        try:\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return float(s.replace(\",\", \".\"))\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    d[\"base_imponible\"] = d[\"base_imponible\"].apply(to_float)\n",
    "    d[\"monto_retencion\"] = d[\"monto_retencion\"].apply(to_float)\n",
    "\n",
    "    # Quitar filas inválidas\n",
    "    d = d.dropna(subset=[\"id_cliente\", \"fecha_emision\"])\n",
    "\n",
    "    # Inserción por lotes\n",
    "    total = 0\n",
    "    records = list(\n",
    "        d[[\"id_cliente\", \"no_secuencial_retencion\", \"fecha_emision\", \"cod_retencion\", \"base_imponible\", \"monto_retencion\"]]\n",
    "        .itertuples(index=False, name=None)\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(records), BATCH_SIZE):\n",
    "        chunk = records[i:i+BATCH_SIZE]\n",
    "        cur.executemany(SQL_INSERT, chunk)\n",
    "        conn.commit()\n",
    "        total += len(chunk)\n",
    "\n",
    "    return total\n",
    "\n",
    "def main():\n",
    "    csv_paths = glob.glob(os.path.join(CSV_DIR, \"*.csv\"))\n",
    "    if not csv_paths:\n",
    "        print(f\"No se encontraron .csv en: {CSV_DIR}\")\n",
    "        return\n",
    "\n",
    "    total_files = 0\n",
    "    total_rows = 0\n",
    "    for path in sorted(csv_paths):\n",
    "        try:\n",
    "            inserted = cargar_csv_a_mysql(path)\n",
    "            print(f\"[OK] {os.path.basename(path)} -> {inserted} filas insertadas (duplicados ignorados)\")\n",
    "            total_files += 1\n",
    "            total_rows += inserted\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {os.path.basename(path)}: {e}\")\n",
    "\n",
    "    print(f\"\\nResumen: Archivos procesados={total_files}, Filas insertadas={total_rows}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        try:\n",
    "            cur.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            conn.close()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7665e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id_clientes_codigos_1.csv: leídas=5000, distinct_id=5000, upserts=5000\n",
      "[OK] id_clientes_codigos_2.csv: leídas=7000, distinct_id=7000, upserts=7000\n",
      "[OK] id_clientes_codigos_3.csv: leídas=500, distinct_id=500, upserts=500\n",
      "[OK] id_clientes_codigos_4.csv: leídas=1500, distinct_id=1500, upserts=1500\n",
      "\n",
      "=== RESUMEN ===\n",
      "Archivos procesados:   4\n",
      "Filas leídas totales:  14000\n",
      "Distinct por archivo:  14000  (nota: suma por-archivo; no equivale a distintos globales)\n",
      "UPSERTs ejecutados:    14000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import mysql.connector as mc\n",
    "\n",
    "# ========= CONFIG =========\n",
    "CSV_DIR = r\"C:\\Users\\santi\\OneDrive - Universidad San Francisco de Quito\\Ingenieria_datos\\Proyecto\\Data\\info_clientes\"\n",
    "BATCH_SIZE = 5000  # tamaño de lote para executemany\n",
    "DB = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"miclave\",\n",
    "    \"database\": \"retenciones_mensuales\",   # <--- según lo que indicaste\n",
    "}\n",
    "TABLE = \"info_clientes\"\n",
    "# ==========================\n",
    "\n",
    "# SQL de UPSERT: mantiene el id_cliente y reemplaza los datos con lo último que entre\n",
    "SQL_UPSERT = f\"\"\"\n",
    "INSERT INTO {TABLE} (\n",
    "  id_cliente,\n",
    "  cod_tipo_id,\n",
    "  cod_provincia\n",
    ") VALUES (%s, %s, %s)\n",
    "ON DUPLICATE KEY UPDATE\n",
    "  cod_tipo_id = VALUES(cod_tipo_id),\n",
    "  cod_provincia = VALUES(cod_provincia);\n",
    "\"\"\"\n",
    "\n",
    "def cargar_csv(path, cur, conn) -> dict:\n",
    "    \"\"\"\n",
    "    Carga un CSV a la tabla info_clientes con UPSERT.\n",
    "    Devuelve métricas de carga.\n",
    "    \"\"\"\n",
    "    # Leer como texto para no perder ceros/formatos\n",
    "    df = pd.read_csv(path, dtype=str, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Normalizar nombres de columnas a minúsculas para mapear robustamente\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # Mapear posibles variantes de encabezados a los nombres destino\n",
    "    # (ajusta si tus CSV usan exactamente estos nombres ya)\n",
    "    rename_map = {}\n",
    "    # id_cliente\n",
    "    for cand in [\"id_cliente\", \"cliente_id\", \"id\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"id_cliente\"\n",
    "            break\n",
    "    # cod_tipo_id\n",
    "    for cand in [\"cod_tipo_id\", \"tipo_id\", \"codigo_tipo_id\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"cod_tipo_id\"\n",
    "            break\n",
    "    # cod_provincia\n",
    "    for cand in [\"cod_provincia\", \"provincia\", \"codigo_provincia\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"cod_provincia\"\n",
    "            break\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Validar columnas requeridas\n",
    "    requeridas = [\"id_cliente\", \"cod_tipo_id\", \"cod_provincia\"]\n",
    "    faltantes = [c for c in requeridas if c not in df.columns]\n",
    "    if faltantes:\n",
    "        raise ValueError(f\"Faltan columnas requeridas {faltantes} en {os.path.basename(path)}\")\n",
    "\n",
    "    # Limpiezas/normalizaciones mínimas\n",
    "    d = df[requeridas].copy()\n",
    "\n",
    "    # Quitar espacios y asegurar strings\n",
    "    for c in requeridas:\n",
    "        d[c] = d[c].astype(str).str.strip()\n",
    "\n",
    "    # Respetar tus longitudes: id_cliente (<=21), códigos (<=2)\n",
    "    d[\"id_cliente\"] = d[\"id_cliente\"].str[:21]\n",
    "    d[\"cod_tipo_id\"] = d[\"cod_tipo_id\"].str[:2]\n",
    "    d[\"cod_provincia\"] = d[\"cod_provincia\"].str[:2]\n",
    "\n",
    "    # Quitar filas claramente inválidas (id vacío)\n",
    "    d = d[d[\"id_cliente\"].notna() & (d[\"id_cliente\"] != \"\")]\n",
    "    d = d[d[\"cod_tipo_id\"].notna() & (d[\"cod_tipo_id\"] != \"\")]\n",
    "    d = d[d[\"cod_provincia\"].notna() & (d[\"cod_provincia\"] != \"\")]\n",
    "\n",
    "    total_leidas = len(d)\n",
    "\n",
    "    # Si el mismo CSV tiene duplicados por id_cliente, conservar la ÚLTIMA aparición (última fila)\n",
    "    d = d.drop_duplicates(subset=[\"id_cliente\"], keep=\"last\")\n",
    "\n",
    "    # Preparar tuplas para inserción\n",
    "    rows = list(d[[\"id_cliente\", \"cod_tipo_id\", \"cod_provincia\"]].itertuples(index=False, name=None))\n",
    "\n",
    "    # Inserción por lotes con UPSERT\n",
    "    insertadas = 0\n",
    "    for i in range(0, len(rows), BATCH_SIZE):\n",
    "        chunk = rows[i:i+BATCH_SIZE]\n",
    "        cur.executemany(SQL_UPSERT, chunk)\n",
    "        conn.commit()\n",
    "        insertadas += len(chunk)\n",
    "\n",
    "    return {\n",
    "        \"archivo\": os.path.basename(path),\n",
    "        \"leidas\": total_leidas,\n",
    "        \"distinct_por_id\": len(d),\n",
    "        \"upserts_ejecutados\": insertadas\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    conn = mc.connect(**DB)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    csvs = sorted(glob.glob(os.path.join(CSV_DIR, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        print(f\"No se encontraron .csv en: {CSV_DIR}\")\n",
    "        cur.close(); conn.close()\n",
    "        return\n",
    "\n",
    "    resumen = []\n",
    "    for p in csvs:\n",
    "        try:\n",
    "            stats = cargar_csv(p, cur, conn)\n",
    "            resumen.append(stats)\n",
    "            print(f\"[OK] {stats['archivo']}: leídas={stats['leidas']}, \"\n",
    "                  f\"distinct_id={stats['distinct_por_id']}, upserts={stats['upserts_ejecutados']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {os.path.basename(p)}: {e}\")\n",
    "\n",
    "    # Resumen global\n",
    "    if resumen:\n",
    "        tot_arch = len(resumen)\n",
    "        tot_leidas = sum(r[\"leidas\"] for r in resumen)\n",
    "        tot_distinct = sum(r[\"distinct_por_id\"] for r in resumen)  # por archivo\n",
    "        tot_upserts = sum(r[\"upserts_ejecutados\"] for r in resumen)\n",
    "        print(\"\\n=== RESUMEN ===\")\n",
    "        print(f\"Archivos procesados:   {tot_arch}\")\n",
    "        print(f\"Filas leídas totales:  {tot_leidas}\")\n",
    "        print(f\"Distinct por archivo:  {tot_distinct}  (nota: suma por-archivo; no equivale a distintos globales)\")\n",
    "        print(f\"UPSERTs ejecutados:    {tot_upserts}\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf646c0",
   "metadata": {},
   "source": [
    "Correr archivo de mysql_logstash\n",
    "\n",
    "cd \"C:\\ELK\\logstash-7.17.10-windows-x86_64\\logstash-7.17.10\\bin\"\n",
    "\n",
    "C:\\ELK\\logstash-7.17.10-windows-x86_64\\logstash-7.17.10\\bin\\logstash.bat -f C:\\ELK\\logstash-7.17.10-windows-x86_64\\logstash-7.17.10\\bin\\mysql_logstash.conf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
